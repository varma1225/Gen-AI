
## RAG Application

## 1.Data Ingestion 

here we loads documents, splits them into chunks, generates embeddings using a  model, and stores them in  Vector Database.


1.  READING: The code goes into your "docs" folder and picks up every 
    text file you've put there.
2.  CUTTING: It chops long documents into small "chunks." Instead of 
    one big 10-page file, it makes many small 1-page snippets. This 
    makes searching much faster.
3.  TRANSLATING (Embedding): It turns those words into "Numbers." 
    Computers are bad at reading words but great at comparing numbers.
4.  SAVING: It sends those numbers and text snippets to your 
    MongoDB database so they are saved forever.


here we have used 

##langchain framework :LangChain framework, which simplifies the development of Retrieval-Augmented Generation (RAG) applications by providing modular components for document loading, text splitting, embeddings, vector storage, and retrieval.

---
## 2. Retrival 

This script verifies whether the vector database can correctly retrieve the most relevant document chunks for a given user query.
It acts as a semantic search validation step before connecting the system to an LLM.


User Question
      ↓
Convert Question to Embedding
      ↓
MongoDB Vector Search
      ↓
Retrieve Top Matching Chunks
      ↓
Display Retrieved Snippets

